# ğŸ§  ModelServeAI

Professional AI Model Serving System with MLflow tracking and Prometheus monitoring.

## ğŸš€ Features

- **FastAPI REST API** - High-performance model serving
- **MLflow Integration** - Experiment tracking and model versioning  
- **Prometheus + Grafana** - Real-time monitoring and metrics
- **Docker Support** - Easy deployment and scaling
- **CI/CD Pipeline** - Automated testing and deployment
- **Health Checks** - System reliability monitoring

## ğŸ“‹ Prerequisites

- Python 3.10+
- Docker & Docker Compose
- Git

## ğŸ› ï¸ Quick Start

### 1. Clone Repository
```bash
git clone https://github.com/Aqilrmm/ModelServeAI.git
cd ModelServeAI
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Train Model
```bash
python app/predict.py
```

### 4. Run Application
```bash
# Development
uvicorn app.main:app --reload --port 8000

# Production with Docker Compose
docker-compose up -d
```

## ğŸ”— Endpoints

- **API**: http://localhost:8000
- **Documentation**: http://localhost:8000/docs
- **MLflow UI**: http://localhost:5000
- **Prometheus**: http://localhost:9090
- **Grafana**: http://localhost:3000 (admin/admin)

## ğŸ“Š API Usage

### Prediction Request
```bash
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": 123,
    "age": 25,
    "gender": "M", 
    "category_preference": "Electronics",
    "price_range": "Medium"
  }'
```

### Response
```json
{
  "user_id": 123,
  "recommendations": [
    {
      "product_id": "product_1",
      "score": 0.85
    }
  ],
  "confidence": 0.85,
  "timestamp": "1640995200.0"
}
```

## ğŸ³ Docker Deployment

```bash
# Build image
docker build -t modelserveai .

# Run container
docker run -p 8000:8000 modelserveai

# Full stack with monitoring
docker-compose up -d
```

## ğŸ“ˆ Monitoring

### Grafana Dashboard
1. Access Grafana at http://localhost:3000
2. Login: admin/admin
3. Import dashboard from `monitoring/grafana_dashboard.json`

### Available Metrics
- Request rate and latency
- Model prediction success rate  
- System health status
- MLflow experiment tracking

## ğŸ§ª Testing

```bash
# Run tests
pytest tests/ -v

# With coverage
pytest tests/ --cov=app --cov-report=html
```

## ğŸ”„ CI/CD

GitHub Actions pipeline includes:
- Automated testing
- Docker image building
- Model training validation
- Deployment automation

## ğŸ“ Project Structure

```
ModelServeAI/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py         # FastAPI application
â”‚   â”œâ”€â”€ predict.py      # Model training logic
â”‚   â”œâ”€â”€ utils.py        # Helper functions
â”‚   â””â”€â”€ model.pkl       # Trained model
â”œâ”€â”€ tests/              # Test suite
â”œâ”€â”€ monitoring/         # Prometheus & Grafana configs
â”œâ”€â”€ .github/workflows/  # CI/CD pipeline
â”œâ”€â”€ Dockerfile         # Container configuration
â”œâ”€â”€ docker-compose.yml # Multi-service setup
â””â”€â”€ requirements.txt   # Dependencies
```

## ğŸ¤ Contributing

1. Fork repository
2. Create feature branch
3. Make changes with tests
4. Submit pull request

## ğŸ“„ License

MIT License - see LICENSE file for details.

---

**ModelServeAI** - Production-ready AI model serving made simple! ğŸ¯